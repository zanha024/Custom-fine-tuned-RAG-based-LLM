{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1XitzHFoYg466zTQhJCc_20RPQL8GOEsh","timestamp":1719394168430},{"file_id":"1AD1QlHy5Qh9s7wmJX9JS0zi2p7uyw-jd","timestamp":1719393491352},{"file_id":"181QHaKInI5wNDPyfIj2sN9YWZBN-VSjW","timestamp":1718769848593}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Fine-tune and quantize LLM in Google Colab using Q-LoRA\n","\n"],"metadata":{"id":"OSHlAbqzDFDq"}},{"cell_type":"code","source":["!git clone https://github.com/zanha024/GenAI-Project\n","%cd GenAI-Project"],"metadata":{"id":"FHD08_Ks2VNT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1719641382096,"user_tz":-330,"elapsed":1511,"user":{"displayName":"Zanha Fathima","userId":"13146891818901560406"}},"outputId":"312a23fc-0a2d-485c-b0d9-3e46103ed734"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'GenAI-Project'...\n","remote: Enumerating objects: 19, done.\u001b[K\n","remote: Counting objects: 100% (19/19), done.\u001b[K\n","remote: Compressing objects: 100% (17/17), done.\u001b[K\n","remote: Total 19 (delta 2), reused 0 (delta 0), pack-reused 0\u001b[K\n","Receiving objects: 100% (19/19), 166.82 KiB | 4.17 MiB/s, done.\n","Resolving deltas: 100% (2/2), done.\n","/content/GenAI-Project/GenAI-Project\n"]}]},{"cell_type":"code","execution_count":12,"metadata":{"id":"GLXwJqbjtPho","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e35523f3-f401-4d09-b7e8-7771c98914ca","executionInfo":{"status":"ok","timestamp":1719641407936,"user_tz":-330,"elapsed":25422,"user":{"displayName":"Zanha Fathima","userId":"13146891818901560406"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.31.0)\n","Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.11.1)\n","Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.43.1)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n","Requirement already satisfied: trl in /usr/local/lib/python3.10/dist-packages (0.9.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.0+cu121)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.4)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from trl) (2.20.0)\n","Requirement already satisfied: tyro>=0.5.11 in /usr/local/lib/python3.10/dist-packages (from trl) (0.8.5)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.20.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.5.40)\n","Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (0.16)\n","Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (13.7.1)\n","Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (1.7.1)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (16.1.0)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (0.6)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (2.0.3)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (3.4.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (0.70.16)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (3.9.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (4.0.3)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.16.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl) (2024.1)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->trl) (1.16.0)\n"]}],"source":["!pip install  accelerate peft bitsandbytes transformers trl"]},{"cell_type":"code","source":["!pip install wandb"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"29Vs2RAgKB9P","executionInfo":{"status":"ok","timestamp":1719641424525,"user_tz":-330,"elapsed":16593,"user":{"displayName":"Zanha Fathima","userId":"13146891818901560406"}},"outputId":"538762be-5b2a-42b7-89f1-6b33ef0c481f"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.17.3)\n","Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n","Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n","Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n","Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.2)\n","Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n","Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n","Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.7.1)\n","Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n","Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.6.2)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"]}]},{"cell_type":"code","source":["!wandb login 7fd97e97a7f81c42d11b3aceb1afe1f67fbb00ad"],"metadata":{"id":"IRhgl93I10d1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1719641430149,"user_tz":-330,"elapsed":5634,"user":{"displayName":"Zanha Fathima","userId":"13146891818901560406"}},"outputId":"05e95a5c-030c-452c-8712-85e9f7fd4662"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]}]},{"cell_type":"code","source":["!pip install llama-cpp-python\n","#pip install llama-cpp-python"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-dGaBBNJwE8s","outputId":"71619ee4-3654-43ec-e6d6-51d466493cf3","executionInfo":{"status":"ok","timestamp":1719641445213,"user_tz":-330,"elapsed":15082,"user":{"displayName":"Zanha Fathima","userId":"13146891818901560406"}}},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: llama-cpp-python in /usr/local/lib/python3.10/dist-packages (0.2.79)\n","Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.12.2)\n","Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.25.2)\n","Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (5.6.3)\n","Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n"]}]},{"cell_type":"code","source":["import os\n","import torch\n","from datasets import load_dataset\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    HfArgumentParser,\n","    TrainingArguments,\n","    pipeline,\n","    logging,\n",")\n","from peft import LoraConfig, PeftModel\n","from trl import SFTTrainer"],"metadata":{"id":"nAMzy_0FtaUZ","executionInfo":{"status":"ok","timestamp":1719641445214,"user_tz":-330,"elapsed":33,"user":{"displayName":"Zanha Fathima","userId":"13146891818901560406"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["## XLSX TO CSV"],"metadata":{"id":"-E3W51I-6c6P"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# Read the XLSX file\n","xlsx_file = pd.read_excel('/content/GenAI-Project/GenAI-Project/crops dataset.xlsx')\n","\n","# Convert XLSX to CSV\n","xlsx_file.to_csv('/content/GenAI-Project/GenAI-Project/crops dataset.xlsx', index=False)"],"metadata":{"id":"FsG6W9OToNf1","executionInfo":{"status":"ok","timestamp":1719641509190,"user_tz":-330,"elapsed":387,"user":{"displayName":"Zanha Fathima","userId":"13146891818901560406"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["# @title prepare data\n","\n","input_prompt = \"\"\"Below is a Human Input, write appropriate Response based on the input.\n","\n","### Input:\n","{}\n","\n","### Response:\n","{}\"\"\"\n"],"metadata":{"id":"gQ889ixtala8","executionInfo":{"status":"ok","timestamp":1719641513008,"user_tz":-330,"elapsed":442,"user":{"displayName":"Zanha Fathima","userId":"13146891818901560406"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["## Detailed Explanation of Fine-Tuning Parameters:\n","\n","This script defines various parameters for fine-tuning a pre-trained model using Low-Rank Adapters (LoRA) and quantization techniques. Here's a breakdown of each section and its role in fine-tuning:\n","\n","**Model and Dataset:**\n","\n","* `model_name`: This specifies the pre-trained model you want to use for fine-tuning. Here, it's set to \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" from the Hugging Face hub.\n","* `new_model`: This defines the name you'll give to the fine-tuned model after training (here, \"tiny-llama-fine-tuned\").\n","\n","**LoRA Parameters:**\n","\n","* `lora_r`: This defines the dimension of the LoRA projection space. It controls the size of the additional parameters introduced for adaptation with LoRA.\n","* `lora_alpha`: This parameter controls the scaling applied to the LoRA weights during training.\n","* `lora_dropout`: This sets the dropout probability for the LoRA layers, helping to prevent overfitting.\n","\n","**BitsAndBytes Parameters (Quantization):**\n","\n","* `use_4bit`: This activates 4-bit precision for loading the base model, potentially reducing model size and inference speed.\n","* `bnb_4bit_compute_dtype`: This sets the computation data type for the 4-bit model (here, \"float16\").\n","* `bnb_4bit_quant_type`: This specifies the type of quantization used (here, \"nf4\").\n","* `use_nested_quant`: This enables nested quantization (double quantization), which might further reduce memory usage but could impact accuracy.\n","\n","**TrainingArguments Parameters:**\n","\n","* `output_dir`: This defines the directory where the model's predictions and checkpoints are saved during training (\"./results\" here).\n","* `num_train_epochs`: This sets the number of training epochs (iterations over the entire dataset). Here, it's set to 50.\n","* `fp16`, `bf16`: These enable mixed-precision training using 16-bit floating-point (fp16) or bfloat16 data types, potentially accelerating training on compatible hardware (set to False here).\n","* `per_device_train_batch_size`: This defines the number of training examples processed per GPU during each training step (set to 1 here). Similarly, `per_device_eval_batch_size` defines the batch size for evaluation.\n","* `gradient_accumulation_steps`: This accumulates gradients for multiple training steps before updating the model weights, potentially improving memory efficiency (set to 1 here).\n","* `gradient_checkpointing`: Enables gradient checkpointing, which saves memory by only storing a subset of activations during backpropagation (enabled here).\n","* `max_grad_norm`: This sets the maximum gradient norm for gradient clipping, preventing exploding gradients (set to 0.3 here).\n","* `learning_rate`: This defines the initial learning rate for the optimizer (AdamW here, set to 2e-4).\n","* `weight_decay`: This applies weight decay (L2 regularization) to all layers except bias and LayerNorm weights, helping to prevent overfitting (set to 0.001 here).\n","* `optim`: This specifies the optimizer used for training. Here, it's set to \"paged_adamw_32bit\".\n","* `lr_scheduler_type`: This defines the learning rate schedule. Here, \"cosine\" is used, which gradually reduces the learning rate over training.\n","* `max_steps`: This sets the total number of training steps (overrides `num_train_epochs`). Here, it's set to -1, meaning all epochs will be used.\n","* `warmup_ratio`: This defines the portion of training steps for a linear warmup of the learning rate (set to 0.03 here).\n","* `group_by_length`: This groups sequences of similar lengths into batches, improving memory efficiency and training speed (enabled here).\n","* `save_steps`: This sets the number of training steps between saving model checkpoints (set to 0 here, meaning no intermediate saves).\n","* `logging_steps`: This defines the number of training steps between logging training information (set to 25 here).\n","\n","**SFT Parameters:**\n","\n","* `max_seq_length`: This sets the maximum sequence length for training and inference (can be left as None).\n","* `packing`: This enables packing multiple short examples into a single input sequence to improve efficiency (disabled here).\n","* `device_map`: This defines which GPUs to use for training. Here, it maps all training to GPU 0 (\"\": 0).\n","\n","These parameters allow you to fine-tune the pre-trained model for a"],"metadata":{"id":"EvM7SaTx9Waf"}},{"cell_type":"code","source":["# The model that you want to train from the Hugging Face hub\n","model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n","#model_mame = \"/content/final_weights_new\"\n","\n","# The instruction dataset to use\n","#dataset_name = \"mlabonne/guanaco-llama2-1k\"\n","\n","# Fine-tuned model name\n","new_model = \"tiny-llama-fine-tuned\"\n","\n","################################################################################\n","# QLoRA parameters\n","################################################################################\n","\n","# LoRA attention dimension\n","lora_r = 64\n","\n","# Alpha parameter for LoRA scaling\n","lora_alpha = 16\n","\n","# Dropout probability for LoRA layers\n","lora_dropout = 0.1\n","\n","################################################################################\n","# bitsandbytes parameters\n","################################################################################\n","\n","# Activate 4-bit precision base model loading\n","use_4bit = True\n","\n","# Compute dtype for 4-bit base models\n","bnb_4bit_compute_dtype = \"float16\"\n","\n","# Quantization type (fp4 or nf4)\n","bnb_4bit_quant_type = \"nf4\"\n","\n","# Activate nested quantization for 4-bit base models (double quantization)\n","use_nested_quant = False\n","\n","################################################################################\n","# TrainingArguments parameters\n","################################################################################\n","\n","# Output directory where the model predictions and checkpoints will be stored\n","output_dir = \"./results\"\n","\n","# Number of training epochs\n","num_train_epochs = 20\n","\n","# Enable fp16/bf16 training (set bf16 to True with an A100)\n","fp16 = False\n","bf16 = False\n","\n","# Batch size per GPU for training\n","per_device_train_batch_size = 1\n","\n","# Batch size per GPU for evaluation\n","per_device_eval_batch_size = 1\n","\n","# Number of update steps to accumulate the gradients for\n","gradient_accumulation_steps = 1\n","\n","# Enable gradient checkpointing\n","gradient_checkpointing = True\n","\n","# Maximum gradient normal (gradient clipping)\n","max_grad_norm = 0.3\n","\n","# Initial learning rate (AdamW optimizer)\n","learning_rate = 2e-4 #0.0002 2x10-4\n","\n","# Weight decay to apply to all layers except bias/LayerNorm weights\n","weight_decay = 0.001\n","\n","# Optimizer to use\n","optim = \"paged_adamw_32bit\"\n","\n","# Learning rate schedule\n","lr_scheduler_type = \"cosine\"\n","\n","# Number of training steps (overrides num_train_epochs)\n","max_steps = -1\n","\n","# Ratio of steps for a linear warmup (from 0 to learning rate)\n","warmup_ratio = 0.03\n","\n","# Group sequences into batches with same length\n","# Saves memory and speeds up training considerably\n","group_by_length = True\n","\n","# Save checkpoint every X updates steps\n","save_steps = 0\n","\n","# Log every X updates steps\n","logging_steps = 25\n","\n","################################################################################\n","# SFT parameters\n","################################################################################\n","\n","# Maximum sequence length to use\n","max_seq_length = None\n","\n","# Pack multiple short examples in the same input sequence to increase efficiency\n","packing = False\n","\n","# Load the entire model on the GPU 0\n","device_map = {\"\": 0} # \"auto\""],"metadata":{"id":"ib_We3NLtj2E","executionInfo":{"status":"ok","timestamp":1719641518517,"user_tz":-330,"elapsed":448,"user":{"displayName":"Zanha Fathima","userId":"13146891818901560406"}}},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":["#Fine-tuning\n","Parameter-efficient fine-tuning (PEFT) is a technique used to adapt large pre-trained language models (LLMs) to new tasks while significantly reducing the number of parameters that need to be trained. Here's a breakdown of the key points:\n","\n","**Challenge of Fine-Tuning LLMs:**\n","\n","* LLMs are massive, with billions of parameters.\n","* Fine-tuning them on new tasks often requires training all these parameters, which can be:\n","    * Computationally expensive (takes a long time and requires powerful hardware).\n","    * Prone to overfitting (the model memorizes the training data instead of learning generalizable patterns).\n","\n","**PEFT Approach:**\n","\n","PEFT addresses these challenges by focusing on training only a small subset of the model's parameters while keeping the rest frozen. This allows for:\n","\n","* **Faster Training:** Less parameters to train means faster training times.\n","* **Reduced Memory Usage:** Smaller models require less memory on devices.\n","* **Improved Generalizability:** By not retraining everything, PEFT can help prevent overfitting and improve the model's ability to adapt to unseen data.\n","\n","**How PEFT Works:**\n","\n","There are several approaches to PEFT\n","\n","* **Low-Rank Adapters (LoRA):** Introducing a small set of additional parameters that act as \"adapters\" on top of the pre-trained model. These adapters allow the model to adapt to the new task without significantly changing the core parameters.\n","\n","**Benefits of PEFT:**\n","\n","* Enables fine-tuning LLMs on resource-constrained devices (e.g., mobile phones).\n","* Reduces training costs associated with large models.\n","* Can potentially improve the generalizability of the fine-tuned model.\n","\n","**Overall, PEFT is a valuable technique for making LLMs more accessible and adaptable to a wider range of tasks while keeping computational efficiency in mind.**\n","\n","Here's a breakdown of why 4-bit quantization is used and what happens to the vectors:\n","\n","**Why Use 4-Bit Quantization?**\n","\n","The code utilizes 4-bit quantization likely for two main reasons:\n","\n","1. **Reduced Model Size and Memory Usage:** Compared to using 32-bit floating-point numbers (FP32) for representing model weights and activations, 4-bit quantization (4 bits per number) significantly reduces the model size. This can be crucial for deploying the model on devices with limited memory, such as mobile phones or embedded systems.\n","\n","2. **Potentially Faster Inference:** While not guaranteed, using lower precision formats like 4-bit can sometimes lead to faster inference speeds on hardware that supports such operations efficiently. This can be beneficial for real-time applications where quick response times are important.\n","\n","**Is it Quantization-Aware Fine-Tuning?**\n","\n","The code snippet doesn't explicitly show if it's using quantization-aware fine-tuning. However, there are clues suggesting it might be:\n","\n","* **`BitsAndBytesConfig`:** This configuration likely controls the quantization settings.\n","* **Target Modules for LoRA:** Fine-tuning only specific modules (like those listed for LoRA) is a common approach when using quantization-aware fine-tuning. This allows for a balance between efficiency gains from quantization and maintaining accuracy.\n","\n","**What Happens to the Vectors During Quantization?**\n","\n","During 4-bit quantization, the original model's weights and activations (represented in FP32) are converted to 4-bit integers. This conversion process involves:\n","\n","1. **Scaling and Clipping:** The FP32 values are first scaled to a specific range suitable for representing with 4 bits. This might involve considering the minimum and maximum values of the original data.\n","2. **Rounding or Quantization:**  A specific strategy is used to convert the scaled values into 4-bit integers. This could involve rounding or other quantization techniques.\n","\n","**Impact on Accuracy:**\n","\n","Quantization, especially aggressive quantization like 4-bit, can introduce some loss of accuracy compared to the original FP32 model. However, the goal is to find a balance between reduced model size/inference speed and acceptable accuracy for the specific task.\n","\n","**Additional Notes:**\n","\n","* The code snippet mentions `bnb_4bit_quant_type` which likely specifies the exact quantization method used (e.g., linear quantization).\n","* The `compute_dtype` (e.g., bfloat16) might be related to the computations performed during training/inference with potentially lower precision formats for further efficiency gains.\n","\n","4-bit quantization aims to reduce model size and potentially speed up inference while considering the trade-off with accuracy."],"metadata":{"id":"67jRa9r788dZ"}},{"cell_type":"code","source":["# Load dataset (you can process it here)\n","#dataset = load_dataset(dataset_name, split=\"train\")\n","\n","# Load tokenizer and model with QLoRA configuration\n","compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=use_4bit,\n","    bnb_4bit_quant_type=bnb_4bit_quant_type,\n","    bnb_4bit_compute_dtype=compute_dtype,\n","    bnb_4bit_use_double_quant=use_nested_quant,\n",")\n","\n","# Check GPU compatibility with bfloat16\n","if compute_dtype == torch.float16 and use_4bit:\n","    major, _ = torch.cuda.get_device_capability()\n","    if major >= 8:\n","        print(\"=\" * 80)\n","        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n","        print(\"=\" * 80)\n","\n","# Load base model\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    quantization_config=bnb_config,\n","    device_map=device_map\n",")\n","model.config.use_cache = False\n","model.config.pretraining_tp = 1\n","\n","# Load LLaMA tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n","EOS_TOKEN = tokenizer.eos_token\n","def formatting_prompts_func(examples):\n","    inputs       = examples[\"Questions\"]\n","    outputs      = examples[\"Answers\"]\n","    texts = []\n","    for input, output in zip(inputs, outputs):\n","        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n","        text = input_prompt.format(input, output) + EOS_TOKEN\n","        texts.append(text)\n","    return { \"text\" : texts, }\n","pass\n","'''\n","def formatting_prompts_func(examples):\n","    inputs       = examples[\"instruction\"]\n","    outputs      = examples[\"output\"]\n","    texts = []\n","    for input, output in zip(inputs, outputs):\n","        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n","        text = input_prompt.format(input, output) + EOS_TOKEN\n","        texts.append(text)\n","    return { \"text\" : texts, }\n","pass'''\n","\n","from datasets import load_dataset\n","dataset = load_dataset('csv', data_files='/content/GenAI-Project/crops dataset.xlsx',split=\"train\")\n","#dataset = load_dataset(\"nmdr/Mini-Physics-Instruct-1k\", split = \"train\")\n","dataset = dataset.map(formatting_prompts_func, batched = True,)\n","# Load LoRA configuration\n","peft_config = LoraConfig(\n","    lora_alpha=lora_alpha,\n","    lora_dropout=lora_dropout,\n","    r=lora_r,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n","      target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n","                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",")\n","\n","# Set training parameters\n","training_arguments = TrainingArguments(\n","    output_dir=output_dir,\n","    num_train_epochs=num_train_epochs,\n","    per_device_train_batch_size=per_device_train_batch_size,\n","    gradient_accumulation_steps=gradient_accumulation_steps,\n","    optim=optim,\n","    save_steps=save_steps,\n","    logging_steps=logging_steps,\n","    learning_rate=learning_rate,\n","    weight_decay=weight_decay,\n","    fp16=fp16,\n","    bf16=bf16,\n","    max_grad_norm=max_grad_norm,\n","    max_steps=max_steps,\n","    warmup_ratio=warmup_ratio,\n","    group_by_length=group_by_length,\n","    lr_scheduler_type=lr_scheduler_type,\n","    report_to=\"wandb\",\n","\n",")\n","\n","# Set supervised fine-tuning parameters\n","trainer = SFTTrainer(\n","    model=model,\n","    train_dataset=dataset,\n","    peft_config=peft_config,\n","    dataset_text_field=\"text\",\n","    max_seq_length=max_seq_length,\n","    tokenizer=tokenizer,\n","    args=training_arguments,\n","    packing=packing,\n","\n",")\n","\n","# Train model\n","trainer.train()\n","\n","# Save trained model\n","trainer.model.save_pretrained(new_model)"],"metadata":{"id":"OJXpOgBFuSrc","colab":{"base_uri":"https://localhost:8080/","height":373},"outputId":"7658d880-194c-48ad-8f6c-ff1f5782bb9e","executionInfo":{"status":"error","timestamp":1719641619907,"user_tz":-330,"elapsed":433,"user":{"displayName":"Zanha Fathima","userId":"13146891818901560406"}}},"execution_count":22,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-50e018701bd7>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Check GPU compatibility with bfloat16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcompute_dtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0muse_4bit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mmajor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device_capability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmajor\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mget_device_capability\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmajor\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mminor\u001b[0m \u001b[0mcuda\u001b[0m \u001b[0mcapability\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m     \"\"\"\n\u001b[0;32m--> 430\u001b[0;31m     \u001b[0mprop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_device_properties\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mprop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmajor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mget_device_properties\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0m_CudaDeviceProperties\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mproperties\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m     \"\"\"\n\u001b[0;32m--> 444\u001b[0;31m     \u001b[0m_lazy_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# will define _get_device_properties\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_device_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptional\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"LAZY\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"]}]},{"cell_type":"code","source":[],"metadata":{"id":"LvjpbE0rnNM-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"8KpjfQP2HXUp"}},{"cell_type":"markdown","source":[],"metadata":{"id":"mjhICnIgHXtJ"}},{"cell_type":"markdown","source":["## Self-Attention with Query, Key, Value\n","\n","Self-attention is a powerful mechanism in transformers that allows the model to focus on relevant parts of the input sequence when processing information. It works with three key components: query, key, and value.\n","\n","**Analogy:** Imagine you're at a party and want to find someone specific (the answer). You (the model) ask everyone at the party a question (the query) to identify potential matches. This question could be \"Are you interested in X?\". Everyone responds with a short description of themselves (the key). You then compare these descriptions to what you're looking for (compare query and key). Finally, you talk to the people whose descriptions seem most relevant (high comparison score) and get more information from them (the value).\n","\n","**Formally:**\n","\n","* **Query (Q):** A vector representing the current focus of attention. It's like your question at the party.\n","* **Key (K):** A vector representing each element in the input sequence. It's like the short description of each person at the party.\n","* **Value (V):** A vector containing the actual information associated with each element in the sequence. It's like the detailed information you get from the relevant people.\n","\n","The model calculates a score for each element in the sequence based on how well its \"key\" matches the \"query.\" Higher scores indicate a better match. Finally, the model uses these scores to weight the \"values\" from each element, creating a new representation that focuses on the most relevant parts of the sequence.\n","\n","**Example:**\n","\n","Consider the sentence \"The cat sat on the mat.\"\n","\n","* **Query:** The query vector could represent the word we're currently focusing on, say \"sat.\"\n","* **Key:** Each word in the sentence would have a key vector. For example, the key vector for \"cat\" might capture its semantic meaning (e.g., furry animal).\n","* **Value:** The value vector for each word would contain its embedding (numerical representation).\n","\n","The model would compare the query vector for \"sat\" with the key vectors of all words. The key vector for \"cat\" might have a higher score than others because \"sat\" often describes actions involving objects that can be sat upon. The model would then use this score to weight the value vector of \"cat,\" giving it more influence in the final representation.\n","\n","## Gate, Up-proj, Down-proj, and O\n","\n","These terms refer to specific parts within a transformer block that process information:\n","\n","**o_proj (Output projection)**:This linear layer is part of the self-attention mechanism. It projects the attention weights (scores for each element) back to the embedding dimension. This allows the model to combine the information from the relevant parts of the sequence into a single representation. This part remains the same as in the regular self-attention mechanism. It's not directly involved in the adaptation process with LoRA.\n","\n","\n","* **Gate:** This is a linear layer within the MLP (multi-layer perceptron) sub-block of a transformer. It takes the hidden state (current representation of the sequence) and projects it to a higher dimension. This creates a more complex representation before applying a non-linear activation function (like ReLU).\n","\n"," `gate_proj`, `up_proj`, and `down_proj` are all part of a transformer block, specifically within the **MLP sub-block**. They perform linear projections on the hidden state, which represents the current understanding of the sequence at that point in processing.\n","\n","Here's a breakdown of their roles and what they project to:\n","\n","* **gate_proj:** This linear layer projects the hidden state (current representation) to a **higher dimension**. This creates a more complex representation by allowing the model to capture a wider range of interactions between elements in the sequence.\n","\n","* **up_proj:** Following the `gate_proj`, this layer further projects the high-dimensional representation to an **even higher dimension**. This allows the model to explore even more intricate relationships within the sequence data.\n","\n","* **down_proj:** Finally, this layer projects the high-dimensional representation obtained from `up_proj` back to the **original embedding dimension**. This essentially compresses the information while still retaining the important details captured in the higher dimensional space.\n","\n","**Overall Flow:**\n","\n","1. The hidden state, representing the current understanding of the sequence, is fed into `gate_proj`.\n","2. `gate_proj` projects it to a higher dimension, creating a more complex representation.\n","3. `up_proj` takes this high-dimensional representation and projects it to an even higher dimension, allowing for exploration of intricate relationships.\n","4. Finally, `down_proj` projects the information back to the original embedding dimension, resulting in a compressed but informative representation.\n","\n","**Where it's Used:**\n","\n","This compressed representation is then fed into the final step of the transformer block, where it's combined with the residual connection (original hidden state) and a layer normalization step. This final output becomes the new hidden state for the next transformer block in the sequence, allowing the model to build a deeper understanding as it processes the entire sequence.\n","\n","**In Summary:**\n","\n","* `gate_proj`, `up_proj`, and `down_proj` are within the **MLP sub-block** of a transformer block.\n","* They project the hidden state to explore complex relationships in the sequence data.\n","* `gate_proj` and `up_proj` project to higher dimensions for more intricate analysis.\n","* `down_proj` projects back to the original dimension for a compressed but informative representation.\n","* This final representation is used to update the hidden state for the next transformer block."],"metadata":{"id":"F9OTa11S8XCb"}},{"cell_type":"code","source":["##Inference\n","inputs = tokenizer(\n","[\n","    input_prompt.format(\n","        \"who developed Nadi?\", # input\n","        \"\",   # leave blank as response generated by AI\n","\n","    )\n","], return_tensors = \"pt\").to(\"cuda\")\n","\n","outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n","generated_text = tokenizer.batch_decode(outputs)[0]\n","first_response = generated_text.split('### Response:')[1].strip()\n","output = first_response.split('###')[0].strip()\n","print(\"the response is: \",output)"],"metadata":{"id":"LELHVNcqvX4T","executionInfo":{"status":"aborted","timestamp":1719641445215,"user_tz":-330,"elapsed":18,"user":{"displayName":"Zanha Fathima","userId":"13146891818901560406"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Reload model in FP16 and merge it with LoRA weights w = w+del(w)\n","base_model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    low_cpu_mem_usage=True,\n","    return_dict=True,\n","    torch_dtype=torch.float16,\n","    device_map=device_map,\n",")\n","model = PeftModel.from_pretrained(base_model, new_model)\n","model = model.merge_and_unload() #W=w+del(w)\n","\n","# Reload tokenizer to save it\n","#tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","#tokenizer.pad_token = tokenizer.eos_token\n","#tokenizer.padding_side = \"right\""],"metadata":{"id":"QQn30cRtAZ-P","executionInfo":{"status":"aborted","timestamp":1719641445216,"user_tz":-330,"elapsed":19,"user":{"displayName":"Zanha Fathima","userId":"13146891818901560406"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["output_dir = \"final_weights_new\"\n","model.save_pretrained(output_dir)"],"metadata":{"id":"rFH8fls61Ns5","executionInfo":{"status":"aborted","timestamp":1719641445216,"user_tz":-330,"elapsed":19,"user":{"displayName":"Zanha Fathima","userId":"13146891818901560406"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Reload tokenizer to save it\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\"\n","tokenizer.save_pretrained(output_dir)"],"metadata":{"id":"2pES7_S11eTw","executionInfo":{"status":"aborted","timestamp":1719641445216,"user_tz":-330,"elapsed":19,"user":{"displayName":"Zanha Fathima","userId":"13146891818901560406"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Huggingface inference of saved model"],"metadata":{"id":"HB6aXWY_M_uB"}},{"cell_type":"code","source":["# Run text generation pipeline with our next model\n","# Load model directly\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"/content/GenAI-Project/GenAI-Project/GenAI-Project/final_weights_new\")\n","model = AutoModelForCausalLM.from_pretrained(\"/content/GenAI-Project/GenAI-Project/GenAI-Project/final_weights_new\")\n","pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=64)\n","prompt=input_prompt.format(\n","        \"what is pest control\", # input\n","        \"\", # leave blank as response generated by AI\n","\n","    )\n","result = pipe(prompt)\n","generated_text  = result[0]['generated_text']\n","\n","first_response = generated_text.split('### Response:')[1].strip()\n","first_response = first_response.split(\"\\n\")[0]\n","\n","print(first_response)"],"metadata":{"id":"Y0widjE9-i9j","executionInfo":{"status":"aborted","timestamp":1719641445216,"user_tz":-330,"elapsed":19,"user":{"displayName":"Zanha Fathima","userId":"13146891818901560406"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Quantization\n","## Key Concepts:\n","**GGUF (Giant GPT Unified Format)**: A model format designed for efficient storage and quantization of large Transformer-based language models like Llama.\n","Llama.cpp: A C++ library for working with GGUF models, including quantization tools.\n","\n","**LoRA (Low-Rank Adaptation)**: A technique for model efficiency and fine-tuning that involves adding adapter layers.\n","\n","**Quantization**: Converting floating-point model weights to lower-precision integers for reduced model size and faster inference.\n","\n","## Quantization Methods:\n","1. **Format Breakdown:**\n","Q#K[S/M/L]:#: Number of bits used (e.g., Q4 = 4 bits).\n","K: Represents low-rank matrix factorization for efficient storage.\n","[S/M/L]: Level of low-rank approximation:S: Small (moderate compression, high precision).\n","M: Medium (balance between compression and precision).\n","L: Large (aggressive compression, lower precision).\n","2. **Conversion Step:**\n","Imagine model weights residing in an apartment complex (FP16 format).\n","Conversion acts like a renovation:Rearrangement: Apartments are grouped and reorganized for efficient processing by quantization tools.\n","Pre-processing: Each apartment gets a thorough cleaning and preparation for the quantization \"paint job.\"\n","No actual quantization happens here; it's all about getting ready for the big transformation.\n","3. **Quantization Step:**\n","Now, the exciting transformation begins!\n","General Process:Calibration: Like measuring wall sizes before applying paint, optimal scaling factors are determined for each weight tensor.\n","Quantization: Weights are meticulously scaled and mapped to specific integer values within a limited range, like assigning each shade a specific paint color.\n","Matrix Factorization (K methods):Think of apartments being replaced with smaller studios (low-rank matrices) for some weights. This saves space and processing power.\n","Not all apartments get shrunk; only those deemed suitable for efficient compression.\n","Fine-tuning: After the renovation, some adjustments are needed. The model is fine-tuned, often using PEFT, to adapt to the quantization-induced \"color shifts\" and maintain accuracy.\n","Merged LoRA Weights:\n","Imagine LoRA adapters as extensions added to the apartment complex. They hold task-specific knowledge.\n","During quantization, these extensions go through the same process as the main building:Rearrangement for efficient processing.\n","Pre-processing for compatibility with quantization.\n","Calibration, scaling, and mapping to specific integer values (colors).\n","Selective matrix factorization for eligible weight tensors.\n","By treating LoRA weights equally, consistency and efficiency are maintained across the entire model after quantization.\n","Choosing the Right Method:\n","It's like picking the perfect renovation plan:Desired Size Reduction: How much do you want to shrink the apartment complex (model)?\n","Accuracy Trade-off: How much \"color change\" can you tolerate?\n","Hardware Compatibility: Will your neighbors (hardware) appreciate the new layout and materials?\n","Fine-tuning Resources: Do you have the tools and time to adjust to the changes?\n","Example: **Q4_K_M Explained:\n","This is like a moderate renovation:Walls get painted with specific \"4-color\" palettes (4-bit quantization).\n","Some rooms are converted into efficient studios (low-rank matrices) for better space utilization.\n","The balance between space saving and accuracy is carefully considered** (medium level of compression).\n","Additional Note:\n","Q8_0 is like keeping some rooms intact (without full quantization). They remain spacious (FP16), offering some size reduction but less efficiency compared to full renovations.\n"],"metadata":{"id":"WuzmlHPn109h"}},{"cell_type":"code","source":["!git clone https://github.com/ggerganov/llama.cpp\n","%cd llama.cpp\n","!make"],"metadata":{"id":"TRFww87h0ee0","executionInfo":{"status":"aborted","timestamp":1719641445216,"user_tz":-330,"elapsed":19,"user":{"displayName":"Zanha Fathima","userId":"13146891818901560406"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd /content/llama.cpp\n","!python3 convert-hf-to-gguf.py /content/GenAI-Project/GenAI-Project/GenAI-Project/final_weights_new --outtype f16"],"metadata":{"id":"l9Cf37JT19VC","executionInfo":{"status":"aborted","timestamp":1719641445216,"user_tz":-330,"elapsed":19,"user":{"displayName":"Zanha Fathima","userId":"13146891818901560406"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!./llama-quantize /content/GenAI-Project/GenAI-Project/GenAI-Project/final_weights_new/ggml-model-f16.gguf /content/GenAI-Project/GenAI-Project/GenAI-Project/final_weights_new/ggml-model-q4_k_m.gguf q4_k_m\n"],"metadata":{"id":"5xh6yr_X2JZo","executionInfo":{"status":"aborted","timestamp":1719641445216,"user_tz":-330,"elapsed":19,"user":{"displayName":"Zanha Fathima","userId":"13146891818901560406"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","from llama_cpp import Llama\n","llm = Llama(model_path=\"/content/GenAI-Project/GenAI-Project/GenAI-Project/final_weights_new/ggml-model-q4_k_m.gguf\",n_gpu_layers=30)\n","prompt = input_prompt.format(\n","        \"what is pest control?\", # input\n","        \"\"              # leave blank as response generated by AI\n","\n","    )\n","\n","output = llm(prompt, max_tokens=200)\n","out = output['choices'][0]['text']\n","generated_text = out\n","first_response = generated_text.split('### Input:')[0].strip()\n","\n","print(first_response)"],"metadata":{"id":"oPS5lwYy2Sb7","executionInfo":{"status":"aborted","timestamp":1719641445217,"user_tz":-330,"elapsed":19,"user":{"displayName":"Zanha Fathima","userId":"13146891818901560406"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"iAE9rICyIsz2","executionInfo":{"status":"aborted","timestamp":1719641445217,"user_tz":-330,"elapsed":19,"user":{"displayName":"Zanha Fathima","userId":"13146891818901560406"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!mv \"/content/GenAI-Project/GenAI-Project/GenAI-Project/final_weights_new/ggml-model-q4_k_m.gguf\" \"/content/drive/MyDrive/GENAI\""],"metadata":{"id":"xwH1pV7yPOPD","executionInfo":{"status":"aborted","timestamp":1719641445217,"user_tz":-330,"elapsed":19,"user":{"displayName":"Zanha Fathima","userId":"13146891818901560406"}}},"execution_count":null,"outputs":[]}]}